\documentclass[11pt,a4paper]{article}

% =========================================================
% PREAMBLE
% =========================================================

% --- Language and Encoding ---
\usepackage[utf8]{inputenc}
\usepackage[T5,T1]{fontenc}
\usepackage[vietnamese,english]{babel}

% --- Layout and Typography ---
\usepackage{geometry}
\geometry{
    top=1in,
    bottom=1in,
    left=1in,
    right=1in,
    headheight=15pt,
    footskip=0.5in
}
\usepackage{microtype}
\usepackage{lmodern}
\usepackage{authblk} 
\usepackage{setspace}
\setstretch{1.2} 
\usepackage{fancyhdr}
\usepackage{titlesec}

% --- Mathematics ---
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{physics} 
\usepackage{stmaryrd} % For specialized brackets

% --- Graphics and Tables ---
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{array}
\usepackage{tabularx}
\usepackage{wrapfig}

% --- Algorithms ---
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{xcolor}

% --- Links and References ---
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hyperref}
\usepackage{url}

% --- Colors & Code Styles ---
\definecolor{linkcolor}{RGB}{0, 50, 150}
\definecolor{deepblue}{RGB}{0, 30, 80}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}

\hypersetup{
    colorlinks=true,
    linkcolor=linkcolor,
    citecolor=linkcolor,
    urlcolor=linkcolor,
    pdftitle={Versor: Isotropic Sequence Modeling},
    pdfauthor={Huy \& Hirst}
}

% --- Mathematical Macros ---
\newcommand{\Cl}{\mathcal{C}l_{4,1}} 
\newcommand{\Spin}{\text{Spin}(4,1)}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bW}{\mathbf{W}} 
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\rev}[1]{\widetilde{#1}}
\newcommand{\grade}[2]{\langle #1 \rangle_{#2}}
\newcommand{\inner}[2]{\langle #1 \rev{#2} \rangle_0}
\newcommand{\wedgeprod}[2]{#1 \wedge #2}
\newcommand{\geo}{\,} % Geometric product (implicit)
\newcommand{\einf}{e_{\infty}}
\newcommand{\eo}{e_{o}}
\newcommand{\RRA}{\text{RRA}}

% --- Header/Footer ---
\pagestyle{fancy}
\fancyhf{}
\rhead{\small \textit{Versor: Isotropic Sequence Modeling}}
\lhead{\small \textit{Huy \& Hirst}}
\cfoot{\thepage}

% =========================================================
% EMBEDDED BIBLIOGRAPHY
% =========================================================
\usepackage{filecontents}
\begin{filecontents}{references.bib}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@book{hestenes2012new,
  title={New foundations for classical mechanics},
  author={Hestenes, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@book{dorst2007geometric,
  title={Geometric algebra for computer science},
  author={Dorst, Leo and Fontijne, Daniel and Mann, Stephen},
  year={2007},
  publisher={Morgan Kaufmann}
}
@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Velickovic, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Katharopoulos, Angelos and others},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@inproceedings{brehmer2023geometric,
  title={Geometric Algebra Transformer},
  author={Brehmer, Johann and De Haan, Pim and Behrmann, Sönke and Cohen, Taco},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}
@inproceedings{ruhe2023clifford,
  title={Clifford Group Equivariant Neural Networks},
  author={Ruhe, David and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}
@inproceedings{sanchez2020learning,
  title={Learning to simulate complex physics with graph networks},
  author={Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle={International Conference on Machine Learning},
  pages={8459--8468},
  year={2020},
  organization={PMLR}
}
@inproceedings{greydanus2019hamiltonian,
  title={Hamiltonian neural networks},
  author={Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}
@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}
@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}
@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
@inproceedings{tillet2019triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}
@inproceedings{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@book{hockney1981computer,
  title={Computer Simulation Using Particles},
  author={Hockney, Roger W and Eastwood, James W},
  year={1981},
  publisher={CRC Press}
}
@misc{pygae2025,
  title={Clifford: Geometric Algebra for Python},
  author={The PyGAE Team},
  year={2025},
  howpublished={\url{https://github.com/pygae/clifford}},
  note={Python library}
}
@inproceedings{satorras2021equivariant,
  title={E(n) equivariant graph neural networks},
  author={Satorras, V{\"\i}ctor Garcia and Ee, Emiel Hoogeboom and Welling, Max},
  booktitle={International conference on machine learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}
@article{sriram2024care,
  title={Clifford Algebraic Rotor Embeddings: Maybe embeddings should start to CARE},
  author={Sriram, Sameeksha and L{\"u}ddecke, Timo and Paliwal, Ayush and Ecker, Alexander S and van de Geijn, Chase},
  journal={arXiv preprint arXiv:2511.11665},
  year={2025}
}
{
@book{doran2003geometric,
    title = {Geometric algebra for physicists},
    author = {Doran, Chris and Lasenby, Anthony},
    year = {2003},
    publisher = {Cambridge University Press}
}
\end{filecontents}

% =========================================================
% DOCUMENT START
% =========================================================

\begin{document}

\selectlanguage{english}

\title{
\LARGE \textbf{Versor:}\\ \Large Conformal Manifold Adapted Sequence Architectures
%\LARGE \textbf{Geometric Sequence Modeling:}\\ \Large Isotropic Sequence Modeling via \texorpdfstring{$\Cl$}{Cl(4,1)} Conformal Manifolds \\ and Recursive Isometries
}

\author[1]{\textbf{\foreignlanguage{vietnamese}{Trương Minh Huy}}}
\author[2]{\textbf{Edward Hirst}}

\affil[1]{Independent Researcher, Da Nang City, Vietnam \protect\\ \texttt{louistruong1111@gmail.com}}
\affil[2]{Instituto de Matemática, Estatística e Computação Científica \protect\\ University of Campinas (UNICAMP), Brazil \protect\\ \texttt{ehirst@unicamp.br}}

\date{February 3, 2026}

\maketitle

\begin{abstract}
\noindent A novel sequence architecture design is introduced, Versor, that uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while maintaining interpretability and efficiency. By embedding states in the $\mathcal{C}l_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents SE(3)-equivariant relationships without requiring explicit structural encoding.

Versor is validated on a variety of tasks: chaotic N-body dynamics, topological reasoning, and multimodal tasks (NLP, Vision, Graph). Across all tasks it outperforms all comparative architectures (...), demonstrating five key advantages: 
\textbf{(1) Structural and Scale Generalization-Versor maintains zero-shot consistency across variable system sizes where standard Euclidean baselines fail due to rigid input dimensions}; 
\textbf{(2) Interpretable and Efficient Representation-}Geometric Product Attention decomposes into scalar (proximity) and bivector (orientational coupling) components, outperforming Graph Networks in accuracy (5.210 vs. 5.881 MSE) and physical stability with 200× fewer parameters than Transformers and 3.9× fewer than Graph Networks; 
\textbf{(3) Multimodal Universality}-Versor achieves 100\% accuracy on geometric vision tasks and stable 3.22 perplexity on NLP, demonstrating its viability as a general-purpose backbone; 
\textbf{(4) Temporal Persistence recursive isometries on the Spin manifold }allow the model to infer hidden momentum from position-only data, achieving a 100× error reduction over frame-based geometric models like GATr; 
\textbf{(5) Linear Complexity}—the Recursive Rotor Accumulator (RRA) achieves O(L) scaling vs. O(L2) for standard attention. 

Versor provides a robust foundation for sequence modeling where scale invariance and interpretability are paramount. 
Results show that by embedding dynamics into the $\mathcal{C}l_{4,1}$ manifold, the model captures physical priors that are more effective than standard relational inductive biases. 
On extreme distribution shifts (OOD Mass), Versor maintains stable predictions (-19.9\% error change) while Transformers experience catastrophic failure (+3097\% error increase). 
Finally, our custom functional kernels achieve up to 78× speedup on GPU backends, providing a scalable path for large-scale improved scientific modeling.

\vspace{0.5em}
\noindent \small{\textbf{Code Availability:} \url{https://github.com/PotatoInfinity/Versor}} \\
\noindent \small{\textbf{DOI:} \href{https://zenodo.org/records/18320794}{10.5281/zenodo.18320794}}
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

The remarkable success of the Transformer architecture \citep{vaswani2017attention} has cemented the ``Sequence of Vectors'' paradigm as the standard for artificial intelligence. Whether the modality is text, images (ViT \citep{dosovitskiy2020image}), or audio, data is tokenized and projected into a flat, high-dimensional Euclidean space ($\R^{d_{model}}$). In this space, relationships between features are modeled via the dot product $\mathbf{q}^T \mathbf{k}$, a scalar measure of similarity.

However, the physical world is not merely a collection of features but a realization of physical laws acting on a structured manifold. Standard neural networks also treat data as points in a flat Euclidean space $\R^d$, relying on the dot product (a measure of similarity) as the primary relational primitive. This approach is \textit{geometrically naive}: it ignores the fundamental symmetries of our universe (rotation, translation, and scaling). To respect these symmetries, a standard Transformer must expend vast computational resources to ``learn'' invariants from millions of augmented examples -- a process that approximates what could be analytically enforced by a simple algebraic group action \citep{bronstein2021geometric}. We term this fundamental mismatch between the geometry of the world and the architecture of the model the \textbf{``Euclidean Bottleneck.''}

This work argues that the next leap in AI Structural Intelligence requires embedding these symmetries directly into the substrate of the network. We present \textbf{Versor}, an architecture built not on linear algebra but on Conformal Geometric Algebra (CGA). Specifically, we utilize $\Cl$, a 5-dimensional framework that linearizes the conformal group of 3D Euclidean space.


\subsection{Key Contributions}
\begin{enumerate}
    \item \textbf{CGA-Based Sequence Model:} First application of Conformal Geometric Algebra ($\Cl$) to temporal sequence modeling, with full architectural specification for processing multivector representations of points, lines, and transformations via recursive isometries.
    
    \item \textbf{Scale Generalization:} Demonstration that geometric priors enable scale-invariant reasoning---Versor achieves 99.3\% MCC on topological connectivity tasks (vs.\ 50.4\% for Vision Transformers) and maintains structural performance across varying sequence densities.
    
    \item \textbf{Interpretable and Efficient Attention:} Geometric Product Attention (GPA) naturally decomposes into scalar (distance-based attraction) and bivector (orientational coupling) components, providing interpretable insights into learned interaction laws with extreme parameter efficiency (200$\times$ savings vs.\ Transformers, 3.9$\times$ vs.\ GNS).
    
    \item \textbf{Recursive Rotor Accumulator (RRA):} A manifold-constrained recurrent mechanism achieving $O(L)$ inference complexity and $O(1)$ memory by representing sequence history as a composite rotation on the Spin manifold, enabling efficient long-sequence modeling on as few as 6,662 parameters.
    
    \item \textbf{Hardware-Efficient Implementation:} Custom Triton and MLX kernels using bit-masked basis contraction to compute Clifford products, achieving up to $78\times$ speedup and eliminating the memory bottleneck (typically $>50\times$ reduction) of standard sparse implementations.
    
    \item \textbf{Multimodal Universality:} Verification that the geometric inductive biases of Versor extend to non-physical domains, achieving \textbf{100\% accuracy} on vision tasks and stable sequential modeling on character-level NLP.
\end{enumerate}

\section{Background: Transformers, CGA, and the Manifold Hypothesis}

\textbf{Transformers and the Euclidean Bottleneck:} Artificial Intelligence is dominated by the Transformer \citep{vaswani2017attention}. While effective, its reliance on flat Euclidean space ($\R^d$) forces it to learn symmetries ($SE(3)$) from data rather than enforcing them algebraically \citep{bronstein2021geometric}. We term this the \textbf{``Euclidean Bottleneck.''}

\textbf{Conformal Geometric Algebra ($\Cl$):} We construct our architecture on $\Cl$, a 32-dimensional algebra generated by $\{e_1, e_2, e_3, e_+, e_-\}$. This framework isometrically lifts 3D points $\bx$ to null vectors $X$ in 5D space:
\begin{equation}
    X = \mathcal{K}(\bx) = \bx + \frac{1}{2}\bx^2 \einf + \eo
    \label{eq:lifting}
\end{equation}
\begin{equation}
    X_i \cdot X_j = -\frac{1}{2}\|\bx_i - \bx_j\|^2
    \label{eq:isometric}
\end{equation}
This ensures that distance calculations are linearized (see Appendix~\ref{app:embedding}). Crucially, transformations are represented uniformly as \textbf{rotors} $R$, which act on state vectors $\Psi$ via the sandwich product $\Psi' = R \Psi \rev{R}$. This structure enforces the \textbf{Manifold Hypothesis}: by limiting latent states to the Spin group, $\Spin \subset \mathcal{C}l^+_{4,1}$, we guarantee valid physical transformations (isometries), preventing unphysical shearing.

For an introductory treatment of Clifford algebra and their applications, we refer to \citep{doran2003geometric}, see Appendix~\ref{app:clifford_intro} for explanations of basis relations and multi-vector structure.

\section{Related Work}

Geometric Deep Learning attempts to encode symmetry priors into neural networks \citep{bronstein2021geometric}, with recent focus on $SO(3)$-equivariant CNNs \citep{cohen2016group} and the Geometric Algebra Transformer (GATr) \citep{brehmer2023geometric}. Unlike static-processing baselines like GATr and CGENN \citep{ruhe2023clifford} which remain ``frame-centric'' and require $O(L^2)$ attention, Versor focuses on \textit{path-based} sequence modeling. By evolving a latent spinor via the Recursive Rotor Accumulator (RRA), Versor captures temporal coherence as a continuous trajectory on a Lie manifold, providing an inductive bias for dynamical systems that standard frame-based models lack.

This approach aligns with sub-quadratic architectures like Mamba \citep{gu2023mamba} and RWKV \citep{peng2023rwkv}, but provides a purely geometric interpretation of the hidden state. While concurrent methods like CARE \citep{sriram2024care} utilize Clifford rotors for positional encodings in Euclidean Transformers, Versor is a complete sequence architecture defined strictly within $\Cl$, achieving $O(L)$ linear scaling. Furthermore, by embedding dynamics directly into the manifold, Versor avoids the need for the manually-enforced governing equations required by PINNs \citep{raissi2019physics} and HNNs \citep{greydanus2019hamiltonian}, while achieving better long-horizon stability than GNS \citep{sanchez2020learning}.

\section{Method: The Versor Architecture}

We propose Versor, a sequence architecture that replaces the vector-space assumptions of Transformers with the graded manifold structure of $\Cl$. The model processes a stream of multivectors via two core mechanisms: Geometric Product Attention (GPA) for relational reasoning and the Recursive Rotor Accumulator (RRA) for temporal integration.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{architecture.png}
    \caption{\textbf{The Versor Architecture.} (Left) Geometric Product Attention (GPA). (Right) The Recursive Rotor Accumulator (RRA).}
    \label{fig:architecture}
\end{figure}

\subsection{Component 1: Geometric Product Attention (GPA)}
Unlike standard attention ($\R^N \to \R$), GPA leverages the algebraic richness of the geometric product. We decompose the product $Q \rev{K}$ into its graded components:
\begin{equation}
    Q \rev{K} = \underbrace{\grade{Q \rev{K}}{0}}_{\text{Scalar (Proximity)}} + \underbrace{\grade{Q \rev{K}}{2}}_{\text{Bivector (Torque)}} + \ \dots
    \label{eq:gpa_decomp}
\end{equation}
We compute attention scores by combining the scalar part (distance-based attraction) with the bivector magnitude:
\begin{equation}
    \alpha_{ij} = \text{softmax}\left(\frac{\grade{Q_i \rev{K}_j}{0} + \gamma \|\grade{Q_i \rev{K}_j}{2}\|}{\sqrt{d_{in}}}\right)
    \label{eq:gpa_score}
\end{equation}
This allows Versor to attend not just to ``how close'' particles are, but ``how they are oriented'' relative to each other (see Figure~\ref{fig:attention_decoding} and Appendix~\ref{app:gpa_decomp} for decomposition proofs).

\subsection{Component 2: Recursive Rotor Accumulator (RRA)}
To achieve $O(L)$ linear scaling ($O(1)$ memory), we replace the quadratic attention matrix with a recursive state $\Psi_t$ constrained to the Spin manifold. At each step $t$, the model predicts a local rotor $\Delta R_t$ (via a Cayley map on the algebra outputs) and updates the global state:
\begin{equation}
    \Psi_{t+1} = \text{Normalize}(\Delta R_t \Psi_t)
    \label{eq:rra_update}
\end{equation}
\textbf{Manifold Normalization:} Crucially, we enforce $\Psi \rev{\Psi} = 1$ at every step. This projects numerical drift back onto the manifold, acting as a geometric regularizer that prevents the ``exploding state'' problem of standard RNNs (see Appendix~\ref{app:stability} for stability proofs). 

\textbf{Hamiltonian Extension:} For strict energy conservation, we optionally employ a \textbf{Hamiltonian-Versor Hybrid} where the network parameterizes an energy surface $H(q,p)$ rather than predicting steps directly (details in Appendix~\ref{app:hamiltonian_extended}).

\section{Hardware Acceleration}
To address the computational cost of the geometric product ($32^2=1024$ operations), we implemented custom kernels in \textbf{OpenAI Triton} \citep{tillet2019triton} and \textbf{Apple MLX}. By exploiting the bits-XOR isomorphism of the Clifford basis, we bypass the memory bottleneck of standard Cayley table lookups. This yields a \textbf{78$\times$} speedup over naive PyTorch implementations and reduces memory overhead by $50\times$. 

\textbf{Latency Note:} While the geometric kernels are extremely efficient, the current end-to-end latency of Versor (14.48 ms) is higher than highly-optimized C++ Transformer baselines (0.82 ms). This is primarily due to the sequential Python loop required for our recurrent RRA implementation, rather than an inherent algebraic limitation. A compiled implementation would bridge this gap while maintaining $O(L)$ scaling. Full algorithmic derivations and complexity proofs are provided in \textbf{Appendix~\ref{app:bitmasked}} and \textbf{\ref{sec:scaling_law}}.

\section{Experiments}
\label{sec:experiments}

\subsection{Chaotic N-Body Dynamics}
We simulate 5 bodies interacting via Newtonian gravity. This system is chaotic (positive Lyapunov exponent).
\textbf{Dataset:} 10k trajectories, $T=1000$ steps using a Leapfrog (Velocity Verlet) integrator \citep{hockney1981computer}. Inputs are positions $\bx$ and velocities $\bv$.
Models are trained using Teacher Forcing but evaluated using \textbf{Autoregressive Rollouts} over a 50-step horizon to ensure physical consistency.
\textbf{Baselines:} 
\begin{itemize}
    \item \textbf{Transformer:} Standard Euclidean self-attention ($d=128$).
    \item \textbf{GNS:} Graph Network Simulator \citep{sanchez2020learning} (Standardized with LayerNorm for numerical stability).
    \item \textbf{HNN:} Hamiltonian NN \citep{greydanus2019hamiltonian} (enforces symplectic gradient flow).
    \item \textbf{Mamba:} State Space Model \citep{gu2023mamba}.
\end{itemize}

\subsubsection{Qualitative Geometric Intelligence}
We decompose GPA into scalar and bivector components (Figure~\ref{fig:attention_decoding}). Scalar maps recover distance laws, while bivector intensity highlights torque-maximizing interactions, validating that Versor attends to the full geometric configuration.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{attention_decoding.png}
    \caption{Geometric Attention Decomposition: Separating Force from Torque. (Left) \textbf{Scalar Attention (Proximity)}: The heatmap of the scalar component $\langle Q \tilde{K} \rangle_0$ recovers the distance-dependent interaction law, prioritizing proximal neighbors (yellow/bright). (Right) \textbf{Bivector Attention (Torque)}: The magnitude of the bivector component $\|\langle Q \tilde{K} \rangle_2\|$ highlights interactions where the ``orientational torque'' is maximal, capturing the angular momentum constraints of the orbital plane. This visualizes how Versor attends to both position and orientation simultaneously.}
    \label{fig:attention_decoding}
\end{figure}

\subsubsection{Long-Horizon Energy Stability}
We rollout the model for $T=200$ steps (training was on $T=50$). We measure the deviation of total energy $H$. Conservation of energy is a proxy for how well the model has learned the underlying physics.

\begin{table}[ht]
\centering
\caption{Performance on Static 5-Body Dynamics (Comparison with State-of-the-Art). Mean $\pm$ std over 5 random seeds. \textbf{Analysis:} Versor achieves superior accuracy (5.21 MSE) compared to both Graph Networks and Transformers while maintaining extreme parameter efficiency.}
\label{tab:n_body_results}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Model & Params & Latency (ms) & MSE ($\downarrow$) & Energy Drift (\%) \\
\midrule
Transformer ($d=128$) & 1.32M & 0.82 & $6.609 \pm 6.415$ & $381.1 \pm 41.1$ \\
Mamba \citep{gu2023mamba} & 0.05M & 1.25 & $7.446 \pm 6.382$ & $238.0 \pm 60.5$ \\
GNS \citep{sanchez2020learning} & 0.026M & 5.50 & $5.881 \pm 6.408$ & $366.7 \pm 85.7$ \\
HNN \citep{greydanus2019hamiltonian} & 0.021M & 8.20 & $4.826 \pm 6.378$ & $10.7 \pm 1.1$ \\
EGNN \citep{satorras2021equivariant} & 0.03M & 9.10 & $6.695 \pm 5.936$ & $723.9 \pm 351.2$ \\
GATr \citep{brehmer2023geometric} & 2.21M & 98.14 & $8.324 \pm 1.805$ & $173.8 \pm 85.8$ \\
\textbf{Versor (Ours)} & \textbf{0.006M} & \textbf{14.48} & $\mathbf{5.210 \pm 6.387}$ & $\mathbf{133.0 \pm 37.7}$ \\
\textbf{Versor (4-ch)} & 0.048M & 16.50 & $6.748 \pm 7.220$ & $187.5 \pm 46.0$ \\
\textbf{Ham-Versor} & 0.044M & 19.80 & $4.827 \pm 6.379$ & $2.4 \pm 1.7$ \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Analysis:} Versor outperforms all baselines in chaotic rollout variance and achieves a $\sim$21\% error reduction over Transformers. Crucially, its energy drift (133\%) is 2.8$\times$ lower than Euclidean models, confirming that manifold constraints stabilize dynamics. The Ham-Versor hybrid achieves SOTA (4.82 MSE, 2.4\% drift). Most importantly, these gains are achieved with \textbf{200$\times$ fewer parameters} than the Transformer baseline.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{scaling_plot.png}
    \caption{Computational Scaling: Latency (ms) vs. Sequence Length $L$. Versor maintains strictly linear $O(L)$ growth, whereas standard Transformers diverge quadratically, reaching memory exhaustion (OOM) at $L=1024$.}
    \label{fig:scaling_plot}
\end{figure}

\textbf{Result:} Versor demonstrates strictly linear $O(L)$ scaling in both latency and memory usage. As shown in Figure~\ref{fig:scaling_plot}, while Euclidean Transformers experience quadratic memory growth and context-window collapse beyond $T=1000$, Versor maintains structural stability across 10,000 timesteps. Although the custom Clifford kernels currently carry a constant-factor overhead compared to highly optimized graph libraries, the architectural guarantee of linear complexity ensures that Versor is uniquely qualified for modeling long-horizon physical trajectories.

\subsection{Generalization Capabilities}
\textbf{Topological Connectivity:} Versor achieves 99.3\% MCC on the ``Broken Snake'' task (vs.\ 50.4\% for ViT), demonstrating that it learns resolution-independent connectivity laws.
\textbf{Variable System Size:} Trained on $N=5$, Versor generalizes zero-shot to $N=3, 7$ with stable error, while Transformers and HNNs fail due to fixed input dimensions.
\textbf{Hidden Velocity:} Without velocity inputs, Versor infers momentum (0.0033 MSE) via recursive state history, whereas the frame-based GATr fails (0.3253 MSE).
\textbf{OOD Mass:} On $10\times$ heavier masses, Versor's error actually improves (\textbf{$-$19.9\%} relative change), while Transformers collapse catastrophically (\textbf{+3097.2\%} error increase).


\subsubsection{Manifold Capacity and Scaling}
To test capacity, we simulated a dense $N=15$ system. A Multi-Channel Versor (48k params) outperformed a Single-Channel baseline (109k params) by 1\%, confirming that parallel Clifford channels effectively mitigate manifold crowding. See Appendix~\ref{app:scaling_capacity} for details.

\subsection{Multimodal Versatility: NLP, Vision, and Graph Tasks}
To demonstrate that Versor is a general-purpose sequence backbone rather than a specialized physics engine, we evaluate it on three non-physical modalities: character-level language modeling (NLP), synthetic image classification (Vision), and geometric graph regression (Graph).

\textbf{NLP (WikiText-2 Valid):} We evaluate character-level modeling on a rapid-validation subset (10k characters) of the WikiText-2 dataset. Versor achieves a mean raw perplexity ($e^{\mathcal{L}}$) of \textbf{3.22 $\pm$ 0.01} (approx.\ 1.69 bits-per-character). While this micro-benchmark does not aim to compete with large-scale SOTA language models ($<1.0$ BPC on full datasets), it confirms that Versor's manifold-constrained transitions can successfully capture semantic sequential structure and converge stably on discrete symbolic data. 

\textbf{Vision (Synthetic CIFAR):} We evaluate spatial feature extraction on a synthetic 32$\times$32 image dataset where classes are defined by distinct geometric features (lines, corners). Versor achieves \textbf{100\% Accuracy (1.0 $\pm$ 0.0)}, confirming its ability to reconstruct spatial hierarchies as multivector orientations.

\textbf{Graph (Geometric Sets):} We evaluate the model's ability to pool unorganized sets of points (representing atoms) into global geometric invariants. Using the Recursive Rotor Accumulator (RRA) for global pooling, Versor achieves a mean MSE of \textbf{1.76 $\pm$ 0.18} on envelope prediction, outperforming standard average-pooling baselines.

These results, aggregated over 5 random seeds, confirm that the geometric inductive biases of Versor provide a robust foundation for universal sequence modeling.

\subsection{Ablation and Analysis}
We conducted rigorous ablation studies (see Appendix~\ref{app:ablation} for full tables) which confirmed: (1) \textbf{Manifold Normalization} is non-negotiable; removing it leads to divergence (NaNs) in chaotic rollouts. (2) The \textbf{Recursive Rotor} mechanism provides the framework for stable trajectory integration. Statistically, Versor provides a Cohen's $d=0.22$ improvement over Transformers with $200\times$ fewer parameters.

% Table moved to appendix

The results demonstrate that the \textbf{Manifold Normalization} step is the primary driver of stability in Clifford-based RNNs. Removing this step leads to rapid divergence (NaN values). The structured evolution on the Spin manifold ensures numerical stability in chaotic regimes where standard architectures fail.

\subsection{Stability and Analysis}
Versor enforces physical stability through Manifold Normalization, resulting in a \textbf{2.9$\times$} improvement in energy conservation over Transformers (133\% vs.\ 381\% drift). We calculate a Cohen's $d=0.22$, indicating a modest but consistent effect achieved with extreme parameter efficiency. Detailed statistical analysis and stability proofs are available in \textbf{Appendix~\ref{app:stats}} and \textbf{\ref{app:stability}}.

\section{Limitations and Future Directions}
Despite our software optimizations, current GPUs remain a Von Neumann bottleneck for GA due to register pressure (32 floats per multivector) and sign-flipping overhead. We propose the \textbf{GAPU (Geometric Algebra Processing Unit)} specification, featuring 1024-bit registers and a systolic Clifford ALU, to neutralize these overheads.

While Versor demonstrates advantages in structural generalization, several limitations remain. Future work should explore optimization directly on the Lie algebra ($\mathfrak{spin}_{4,1}$) to avoid manual normalization, and investigate Riemannian initialization schemes to address the high variance observed across random seeds. Furthermore, integrating symplectic integrators directly into the geometric update rule may allow Versor to satisfy both geometric and physical conservation laws simultaneously.

\subsection{Addressing the Numerical Drift (The ``Lie Algebra'' Path)}
While our current implementation utilizes a retraction-based normalization step to maintain manifold constraints, future work could explore optimization directly on the Lie algebra ($\mathfrak{spin}_{4,1}$). Utilizing the exponential map for updates would theoretically guarantee strict adherence to the manifold without manual normalization, though efficient computation of the multivector exponential remains an open engineering challenge.

\subsection{Addressing the Energy Drift (The ``Hamiltonian'' Path)}
We observe that strict geometric constraints do not strictly imply physical energy conservation. A promising direction for future research is the integration of symplectic integrators or Hamiltonian inductive biases directly into the geometric update rule, potentially allowing Versor to satisfy both geometric and physical conservation laws simultaneously.

\subsection{Addressing the Metric Bias (The ``Riemannian'' Path)}
Our framework currently assumes a flat Euclidean metric via the standard CGA inner product. Extending this to learnable or curvature-dependent metrics (Riemannian Geometric Algebra) would allow the model to generalize to non-Euclidean domains, such as relativistic physics or hyperbolic graph embeddings.

\subsection{Optimization on Lie Manifolds}
While the derived \textbf{Versor Initialization} (Appendix~\ref{app:init_derivation}) successfully ensures isometric signal propagation and prevents the ``Geometric Soup'' phenomenon, the optimization landscape on the Spin manifold remains non-convex and complex. Future work should investigate \textbf{Riemannian Optimization} algorithms (e.g., RSGD) to further reduce training variance and accelerate convergence, moving beyond the retraction-based Adam optimizers currently used.

\subsection{When to Use Versor vs. Alternatives}

Based on our experiments, we recommend:

\textbf{Use Versor when:}
\begin{itemize}
    \item Geometric structure dominates (SE(3) symmetries are critical)
    \item Interpretability is valued (model debugging, scientific discovery)
    \item Sequences are long (exploit $O(L)$ complexity)
    \item Parameter budget is extremely limited (\textbf{200$\times$ smaller} than Transformer).
\end{itemize}

\textbf{Use GNS when:}
\begin{itemize}
    \item Structure is fixed and known a priori
    \item Maximum predictive accuracy is the sole objective
\end{itemize}

\textbf{Use Transformer when:}
\begin{itemize}
    \item Domain has no obvious geometric structure
    \item Conservation laws must be learned implicitly
    \item Computational resources are abundant
\end{itemize}

Versor is not a universal replacement but rather a complementary tool excelling where structural flexibility and interpretability are paramount.

\section{Conclusion}

We introduced Versor, a sequence architecture based on Conformal Geometric Algebra that prioritizes \textit{structural generalization} and \textit{interpretability}. Our empirical validation demonstrates that by evolving recursive states on the Spin manifold, Versor achieves: (1) \textbf{Zero-shot scale generalization} (solving topological tasks invisible to ViTs), (2) \textbf{Physical Interpretability} (separating proximity from torque), and (3) \textbf{Linear Complexity} via the Recursive Rotor Accumulator.

While Versor does not yet match the raw throughput of highly optimized matrix baselines, its ability to model fundamental symmetries ($SE(3)$) without data augmentation positions it as a foundational architecture for \textbf{Structural Intelligence}. Future work will focus on hardware-accelerated geometric processing units and Riemannian initialization schemes to fully realize the potential of algebraic computing.

\section*{Acknowledgments}
The authors wish to thank the open-source community for the Triton language, the Apple MLX team, and the developers of the \texttt{clifford} Python library \citep{pygae2025}; as well as Tomás dos Santos Rodrigues e Silva for comments.
EH is supported by São Paulo Research Foundation (FAPESP) grant 2024/18994-7.



\section*{Reproducibility Statement}
All custom Triton and MLX kernels, dataset generation scripts, and model weights used in this study are available in our public repository. We provide a \texttt{requirements.txt} and a step-by-step \texttt{README} to ensure that every result, from the bit-masked speedups to the chaotic 5-body rollouts, can be replicated on standard hardware. Note: The provided minimal example code uses simplified hyperparameters (LR=$10^{-3}$, constant) for rapid verification, while the reported state-of-the-art results were obtained using the tuned schedule described in Appendix~\ref{app:details}.

\bibliographystyle{plainnat}
\bibliography{references}


\appendix
\section{Introduction to Clifford Algebra and Spin Groups}
\label{app:clifford_intro}
In this section, we provide a self-contained introduction to the algebraic structures used in Versor. For a more rigorous explanation, see \citep{doran2003geometric}
\subsection{Basic Definitions and Basis Relations}
A Clifford algebra $\mathcal{C}l(V, q)$ is an associative algebra generated by a vector space $V$ equipped with a quadratic form $q$. The fundamental relation that defines the algebra is:
\begin{equation}
    v^2 = q(v)1 \quad \forall v \in V
\end{equation}
For a basis $\{e_i\}$ of $V$, this relation implies the anti-commutation rules:
\begin{equation}
    e_i e_j + e_j e_i = 2 \eta_{ij}
\end{equation}
where $\eta_{ij}$ is the metric signature. In the Conformal Geometric Algebra $\Cl$ used in this paper, the basis vectors $\{e_1, e_2, e_3, e_+, e_-\}$ satisfy $e_1^2 = e_2^2 = e_3^2 = e_+^2 = 1$ and $e_-^2 = -1$.

\subsection{Multivectors and the Graded Structure}
The Clifford algebra is a graded vector space. A general element, called a \textbf{multivector}, is a linear combination of basis blades:
\begin{equation}
    A = \underbrace{a_0}_{\text{scalar}} + \underbrace{\sum a_i e_i}_{\text{vectors}} + \underbrace{\sum a_{ij} e_{ij}}_{\text{bivectors}} + \dots + \underbrace{a_{123+-} e_{123+-}}_{\text{psudoscalar}}
\end{equation}
An element formed by the outer product of $k$ vectors is called a $k$-blade, and represents a $k$-dimensional subspace with a specific orientation and magnitude. Versor utilizes this graded structure to represent not just points (vectors), but also lines, circles, and planes (higher-grade blades) within a unified framework.

\subsection{The Spin Group and Geometric Transformations}
A \textbf{versor} is a multivector that can be expressed as the product of non-null vectors. The set of even versors $R$ satisfying $R \rev{R} = 1$ forms the \textbf{Spin Group} $\text{Spin}(V, q)$. This group is a double cover of the special orthogonal group $SO(V, q)$.

The action of a rotor $R \in \text{Spin}(V, q)$ on an object $X$ is given by the \textbf{sandwich product}:
\begin{equation}
    X' = R X \rev{R}
\end{equation}
This transformation is an isometry: it preserves the grade of $X$ and all inner products between elements. Unlike standard linear layers in neural networks which can represent arbitrary shearing and scaling, Versor's latent transitions are restricted to the Spin manifold.

\subsection{Why This Reduction Matters}
Restricting latent state evolutions to the Spin manifold (the "reduction" from arbitrary Clifford elements to versors) provides several critical advantages:
\begin{enumerate}
    \item \textbf{Geometric Integrity:} It ensures that transformations correspond to valid rigid body motions and conformal maps (rotations, translations, scalings), preventing unphysical distortions of the state space.
    \item \textbf{Normalization as Regularization:} The requirement $R \rev{R} = 1$ acts as a built-in regularizer. Projecting numerical drift back onto the manifold prevents signal explosion and ensures stable long-horizon integration.
    \item \textbf{Global Consistency:} Since the Spin group is a Lie group, it allows for continuous, smooth evolution of states, which is essential for modeling dynamical systems where temporal coherence is required.
\end{enumerate}

\section{Mathematical Stability and Geometric Fundamentals}
\label{app:math_fundamentals}

\subsection{Proofs of Stability}
\label{app:stability}

\subsection{Isometric Property of Rotors}

\textbf{Theorem 1:} \textit{The action of a rotor $R$ on a multivector $X$ preserves the scalar product.}
\begin{proof}
Let $X' = R X \rev{R}$. We wish to show $\grade{X' \rev{X}'}{0} = \grade{X \rev{X}}{0}$.
\begin{align}
    X' \rev{X}' &= (R X \rev{R}) (\rev{R X \rev{R}}) \\
    &= R X \rev{R} R \rev{X} \rev{R}
\end{align}
By definition of a rotor, $\rev{R} R = 1$.
\begin{equation}
    = R (X \rev{X}) \rev{R}
\end{equation}
Note that $X \rev{X}$ is a scalar (norm squared) in Euclidean vector spaces, but in CGA it may contain higher grades. However, the scalar part is invariant under rotation:
\begin{equation}
    \grade{R (X \rev{X}) \rev{R}}{0} = \grade{(X \rev{X}) R \rev{R}}{0} = \grade{X \rev{X}}{0}
\end{equation}
Thus, the norm is preserved.
\end{proof}

\subsection{Recurrent Stability Analysis}
Consider the recurrence $\Psi_{t+1} = R_t \Psi_t$.
The Lyapunov exponent $\lambda$ is defined by $\lim_{t \to \infty} \frac{1}{t} \ln \|\Psi_t\|$.
Since $\|R_t\| = 1$ (enforced by our Manifold Normalization step), we have:
\begin{equation}
    \|\Psi_{t+1}\| = \|R_t\| \|\Psi_t\| = 1 \cdot \|\Psi_t\|
\end{equation}
Thus, $\|\Psi_t\| = \|\Psi_0\|$ for all $t$.
\begin{equation}
    \lambda = \lim_{t \to \infty} \frac{1}{t} \ln (1) = 0
\end{equation}
A zero Lyapunov exponent indicates the system is \textbf{marginally stable} (conservative). It effectively solves the exploding/vanishing gradient problem by construction, as the state vector rotates on a hypersphere rather than expanding or contracting in Euclidean space.

\section{Algebraic Computing and Hardware Optimization}
\label{app:hardware_optimization}

\subsection{Derivation of the Bit-Masked Geometric Product}
\label{app:bitmasked}

\textbf{Goal:} Derive a closed-form bitwise expression for the geometric product $C = AB$ for any two basis blades $e_i, e_j$ in the Conformal Geometric Algebra $\Cl$, such that $e_i e_j = \sigma(i, j) \cdot \eta(i, j) \cdot e_{i \oplus j}$. We prove that the geometric product in $C\ell_{4,1}$ is isomorphic to a fused bitwise transformation $\phi(a, b) = (a \oplus b, \text{sgn}(a, b))$, where the sign is a closed-form parity function of the bit-interleaving.

\subsection*{Definition: The Basis Isomorphism $\phi$}
We define an isomorphism $\phi: \mathcal{G} \to \mathbb{Z}_2^n$ between the Grassmann basis $\mathcal{G}$ and the $n$-dimensional bit-field. For any blade $e_S$, $\phi(e_S) = \sum_{k \in S} 2^k$. The geometric product $e_i e_j$ is then mapped to the bitwise domain as:
\begin{equation}
    \phi(e_i e_j) = (\phi(e_i) \oplus \phi(e_j), \text{sgn}(\phi(e_i), \phi(e_j)))
\end{equation}
where $\oplus$ is the bitwise XOR operator and $\text{sgn}$ captures the topological parity and metric signature.


\subsection*{Step 1: Bitmask Representation}
Let the basis vectors $\{e_1, e_2, e_3, e_+, e_-\}$ be mapped to indices $\{0, 1, 2, 3, 4\}$. We represent any basis blade $e_S$ as an integer bitmask $i \in \{0, \dots, 31\}$, where:
\[ i = \sum_{k \in S} 2^k \]
For example, the bivector $e_{12}$ is represented by the bitmask $2^0 + 2^1 = 3$ (binary \texttt{00011}).

\subsection*{Step 2: Target Index Isomorphism}
The geometric product of two blades is defined by the juxtaposition of their basis vectors. Vectors appearing in both blades contract, while unique vectors remain. This is equivalent to the \textbf{Symmetric Difference} of the sets of basis indices.
\textbf{Proof:} 
In set theory, the symmetric difference $S \Delta T$ contains elements in either $S$ or $T$, but not both. In bitmask space, the XOR operator ($\oplus$) is the functional equivalent of the symmetric difference:
\[ \text{index}(e_i e_j) = i \oplus j \]
This proves that the resulting blade's basis is always found at the XORed index, eliminating the need for a search or hash map.

\subsection*{Step 3: The Geometric Sign (Anti-commutativity)}
The sign $\sigma(i, j) \in \{1, -1\}$ arises from the number of swaps required to move all basis vectors in $e_j$ to their canonical positions relative to $e_i$.
\begin{enumerate}
    \item Let $e_i = e_{a_1} e_{a_2} \dots e_{a_m}$ and $e_j = e_{b_1} e_{b_2} \dots e_{b_n}$.
    \item To calculate $e_i e_j$, we move $e_{b_1}$ past all $e_{a_k}$ where $\text{index}(a_k) > \text{index}(b_1)$. Each such move incurs a sign change $(-1)$ due to $e_a e_b = -e_b e_a$.
    \item The total number of swaps $N$ is the count of pairs $(k, l)$ such that $\text{index}(a_k) > \text{index}(b_l)$.
\end{enumerate}

\textbf{Bitwise Optimization:} 
For a fixed bit $b \in j$, the number of bits in $i$ that are ``to the left'' (higher index) of $b$ is given by \texttt{popcount(i \& \textasciitilde( (1 << (b+1)) - 1 ))}. Summing this over all bits in $j$ provides the total swap parity. A more efficient parallel version used in our Triton kernel is:
\[ N_{swaps} = \sum_{k=0}^{n-1} [j_k \cdot \text{popcount}(i \gg (k+1))] \]
The sign is then:
\[ \sigma(i, j) = (-1)^{N_{swaps}} \]

\subsection*{Step 4: Metric Signature Contraction}
The metric $\eta$ defines the result of $e_k^2$. In $\Cl$, the signature is $(1, 1, 1, 1, -1)$.
\begin{enumerate}
    \item Contraction occurs only for basis vectors present in both $i$ and $j$, defined by the bitwise AND: $mask_{intersect} = i \& j$.
    \item For each bit $k$ set in $mask_{intersect}$, we multiply the result by the signature $\eta_{kk}$.
    \item Since $\eta_{kk} = -1$ only for the 5th basis vector ($e_-$), the metric sign $\eta(i, j)$ is simply:
\end{enumerate}
\[ \eta(i, j) = 
\begin{cases} 
-1 & \text{if } (i \ \& \ j \ \& \ 2^4) \neq 0 \\
1 & \text{otherwise}
\end{cases} \]

\subsection*{Step 5: Final Fused Computation}
Combining the above, the coefficient for the product of two multivectors $A$ and $B$ at index $k$ is:
\[ C_k = \sum_{i \oplus j = k} A_i B_j \cdot (-1)^{\text{parity}(i, j)} \cdot \eta(i, j) \]
This formulation allows the GPU to compute the geometric product using only register-local bit-logic, bypassing the $O(d^3)$ memory bottleneck of traditional Cayley tables.

\subsection{Computational Efficiency of the Bit-Masked Kernel}
\label{sec:scaling_law}

Let $n$ be the number of basis vectors (for CGA, $n=5$) and $D = 2^n$ be the total number of basis blades (for CGA, $D=32$).

\subsection{Algorithmic Complexity: $O(D^3)$ vs. $O(D^2 \cdot n)$}

\textbf{The Matrix Method ($T_{matrix}$):}  
Most standard Deep Learning frameworks (PyTorch/TensorFlow) implement GA by representing multivectors as $D \times D$ matrices. The geometric product then becomes a standard matrix multiplication.
\begin{itemize}
    \item \textbf{Operations:} For each of the $D^2$ entries in the resulting matrix, you perform a dot product of length $D$.
    \item \textbf{Complexity:} $D \times D \times D = D^3$.  
    \item For CGA: $32^3 = \mathbf{32,768}$ \textbf{Floating Point Operations (FLOPs).}
\end{itemize}

\textbf{The Bit-Masked Method ($T_{bit}$):}  
We iterate through all pairs of non-zero coefficients. In the dense case, there are $D^2$ pairs. For each pair, we compute the sign and index using bit-logic.
\begin{itemize}
    \item \textbf{Operations:} $D \times D$ iterations. Inside each, you perform $\approx n$ bitwise operations to resolve the sign (parity of the swap).
    \item \textbf{Complexity:} $n \cdot D^2$.
    \item For CGA: $5 \cdot 32^2 = \mathbf{5,120}$ \textbf{Logic/FLOPs.}
\end{itemize}

\textbf{Theoretical Speedup ($\alpha$):}
\[ \alpha = \frac{D^3}{n \cdot D^2} = \frac{D}{n} \]
For CGA ($32/5$), our method reduces the raw number of operations by a factor of \textbf{6.4x}.

\subsection{The Memory Wall: Latency Proof}
In modern GPU architectures (Triton/CUDA), math is ``cheap'' but memory is ``expensive.''

\textbf{Cayley Table Method (Standard GA Optimization):}
To avoid $O(D^3)$ math, researchers use a lookup table of size $D \times D$.
\begin{itemize}
    \item \textbf{Access Pattern:} For every product $A_i B_j$, the kernel must fetch \texttt{SignTable[i][j]} and \texttt{IndexTable[i][j]} from memory.
    \item \textbf{Latency:} Even in \textbf{Shared Memory}, you face \textbf{Bank Conflicts}. If 32 threads in a warp access different table indices, the hardware serializes the requests.
    \item \textbf{Cost:} $Time \approx Latency_{Memory} + Time_{FLOP}$.
\end{itemize}

\textbf{The Bit-Masked Method:}
\begin{itemize}
    \item \textbf{Access Pattern:} \textbf{Zero} table lookups. The index and sign are computed using \textbf{Register-local} bitwise instructions (\texttt{xor}, \texttt{and}, \texttt{vpopcnt}).
    \item \textbf{Latency:} ALU instructions have a latency of $\approx 1$ cycle. Memory lookups (even L1 cache) have a latency of $\approx 20$--$80$ cycles.
    \item \textbf{Cost:} $Time \approx n \cdot Latency_{ALU} + Time_{FLOP}$.
\end{itemize}

\textbf{Latency Speedup ($\beta$):}
\[ \beta = \frac{Latency_{Table\_Lookup}}{n \cdot Latency_{ALU}} \approx \frac{40 \text{ cycles}}{5 \cdot 1 \text{ cycle}} \approx \mathbf{8x} \]
Because we compute instead of fetch, we bypass the ``Memory Wall'' entirely.

\subsection{Operational Intensity ($\mathcal{I}$)}
The \textbf{Roofline Model} defines performance based on ``Operations per Byte'' ($\mathcal{I} = Ops / Bytes$).

\begin{itemize}
    \item \textbf{Cayley Method:} To perform 1 Multiply-Accumulate (MAC), you load 2 Floats (8 bytes) + 2 Integers from the table (8 bytes). 
    \[ \mathcal{I}_{cayley} = \frac{1 \text{ op}}{16 \text{ bytes}} = 0.0625 \]
    \item \textbf{The Bit-Masked Method:} You load 2 Floats (8 bytes) and perform the MAC. The bitwise logic is ``free'' because it happens in the ALU while the next data is being pre-fetched.
    \[ \mathcal{I}_{bit} = \frac{1 \text{ op}}{8 \text{ bytes}} = 0.125 \]
\end{itemize}

\textbf{Conclusion:} Our method is \textbf{200\% more efficient} at utilizing the GPU's limited memory bandwidth.

\subsection{Final Cumulative Speedup Calculation}
The industrial speedup is the product of algorithmic reduction and hardware efficiency:

\begin{enumerate}
    \item \textbf{Algorithmic Gain:} $6.4x$ (reduction in total work).
    \item \textbf{Hardware Gain:} $\approx 4x$ to $10x$ (moving from memory-bound table lookups to compute-bound bitwise logic).
\end{enumerate}

\textbf{Total Projected Speedup ($S$):}
\[ S_{total} = \left( \frac{D}{n} \right) \times \left( \frac{Latency_{Mem}}{n \cdot Latency_{ALU}} \right) \]
For \textbf{CGA ($\Cl$)} on modern NVIDIA (Triton) or Apple Silicon (MLX) GPUs:
\[ S_{total} \approx 6.4 \times 8 \approx \mathbf{51.2x} \]

\subsection{Proof of Isometric Conformal Embedding}
\label{app:embedding}
\textbf{Theorem (Eq.~\ref{eq:isometric}):} \textit{The inner product of two null vectors $X_1, X_2 \in \Cl$ corresponds to the negative half-squared Euclidean distance between their original points $x_1, x_2 \in \R^3$.}

\textbf{Proof:}
\begin{enumerate}
    \item \textbf{Recall the Lifting } $\mathcal{K}(\bx)$ (Eq.~\ref{eq:lifting}):
    $X = x + \frac{1}{2}x^2 e_\infty + e_o$, where $e_\infty^2 = 0, e_o^2 = 0,$ and $e_\infty \cdot e_o = -1$.
    \item \textbf{Expand the Inner Product $X_1 \cdot X_2$:}
    $X_1 \cdot X_2 = (x_1 + \frac{1}{2}x_1^2 e_\infty + e_o) \cdot (x_2 + \frac{1}{2}x_2^2 e_\infty + e_o)$
    \item \textbf{Distribute the terms using Minkowski orthogonality:}
    \begin{itemize}
        \item $x_1 \cdot x_2$ (Standard Euclidean dot product)
        \item $x_1 \cdot e_\infty = 0, x_1 \cdot e_o = 0$ (Vectors are orthogonal to null basis)
        \item $e_\infty \cdot e_\infty = 0, e_o \cdot e_o = 0$ (Null vectors)
        \item $(\frac{1}{2}x_1^2 e_\infty) \cdot e_o = \frac{1}{2}x_1^2 (e_\infty \cdot e_o) = -\frac{1}{2}x_1^2$
        \item $e_o \cdot (\frac{1}{2}x_2^2 e_\infty) = \frac{1}{2}x_2^2 (e_o \cdot e_\infty) = -\frac{1}{2}x_2^2$
    \end{itemize}
    \item \textbf{Combine the non-zero results:}
    $X_1 \cdot X_2 = x_1 \cdot x_2 - \frac{1}{2}x_1^2 - \frac{1}{2}x_2^2$
    \item \textbf{Factorize:}
    $X_1 \cdot X_2 = -\frac{1}{2}(x_1^2 + x_2^2 - 2x_1 \cdot x_2) = -\frac{1}{2}\|x_1 - x_2\|^2$
\end{enumerate}

\textbf{Conclusion:} This proves that Versor can calculate distances via linear dot products without ever using a $\sqrt{\cdot}$ or non-linear activation, explaining its efficiency in physics tasks. 

\section{Model Architecture and Representation}
\label{app:architecture_details}

\subsection{Derivation of Versor Initialization}
\label{app:init_derivation}
\textbf{Problem Formulation:}
In a standard neural network, weights are initialized to preserve the variance of activations across layers. For a linear layer $y = Wx$, we require $\text{Var}(y) \approx \text{Var}(x)$. In Versor, the fundamental operation is the Geometric Product between a weight multivector $W$ and a feature multivector $x$:
$$y = W x$$
Since $W$ and $x$ are elements of $\Cl$ (dimension $D=32$), the product $Wx$ involves a summation over the basis elements defined by the Cayley table. We derive the initialization variance $\sigma_w^2$ required to ensure $\mathbb{E}[\|y\|^2] \approx \mathbb{E}[\|x\|^2]$.

\subsection{Derivation}
Let $x = \sum_{i=1}^{D} x_i e_i$ and $W = \sum_{j=1}^{D} w_j e_j$, where $x_i, w_j$ are i.i.d. random variables with mean 0 and variances $\sigma_x^2, \sigma_w^2$. The geometric product is defined as:
$$y = \left( \sum_{j=1}^D w_j e_j \right) \left( \sum_{i=1}^D x_i e_i \right) = \sum_{j,i} w_j x_i (e_j e_i)$$
From the properties of the Cayley table of $\Cl$, for any fixed basis element $e_k$ in the output, and for any input basis $e_i$, there exists exactly one weight basis $e_j$ such that $e_j e_i = \pm e_k$. Thus, each component $y_k$ of the output is a sum of $D$ terms:
$$y_k = \sum_{p=1}^D \delta_p (w_{\pi(p)} x_p)$$
where $\delta_p \in \{-1, 1\}$ is the sign change from the algebra, and $\pi(p)$ maps the indices. Assuming independence, the variance of the sum is the sum of the variances:
$$\text{Var}(y_k) = \sum_{p=1}^D \text{Var}(w_{\pi(p)}) \text{Var}(x_p) = D \cdot \sigma_w^2 \cdot \sigma_x^2$$

\subsection{The Scaling Law}
Notice the factor $D$. In $\Cl$, $D=32$. Standard initialization schemes assume $D=1$ (scalars). If used here, the variance increases by $32\times$ at each layer, leading to signal explosion (``Geometric Soup''). To preserve variance ($\text{Var}(y_k) \approx \text{Var}(x_k)$), we must satisfy:
$$D \cdot \sigma_w^2 = 1 \implies \sigma_w^2 = \frac{1}{D}$$

\subsection{Final Formula}
For a Geometric Linear layer with $fan_{in}$ input channels and algebra dimension $D=32$:
$$\mathcal{W} \sim \mathcal{N}\left(0, \frac{2}{fan_{in} \times 32}\right)$$
(Using the He factor of 2 for ReLU-like non-linearities, or 1 for linear/TanH).

\subsection{Geometric Product Attention (GPA) Decomposition}
\label{app:gpa_decomp}
\textbf{Theorem (Eq.~\ref{eq:gpa_decomp}):} \textit{The GPA attention score $Q \rev{K}$ naturally decomposes into a Proximity Score (Scalar) and an Orientational Torque (Bivector).}

\textbf{Proof:}
\begin{enumerate}
    \item Let $Q, K$ be multivectors in $\Cl$. The full geometric product is $S = Q \rev{K}$.
    \item \textbf{Grade Projection:} By the definition of the Clifford product:
    $Q \rev{K} = \grade{Q \rev{K}}{0} + \grade{Q \rev{K}}{2} + \grade{Q \rev{K}}{4}$
    (Note: Only even grades appear because Query/Key are constructed as rotors or vectors).
    \item \textbf{Scalar Part ($\grade{\cdot}{0}$):}
    From Appendix~\ref{app:embedding}, we know $Q \cdot K = -\frac{1}{2}d^2$. Thus, $\text{softmax}(\grade{Q \rev{K}}{0})$ recovers the standard RBF-like distance attention used in Vanilla Transformers.
    \item \textbf{Bivector Part ($\grade{\cdot}{2}$):}
    $\grade{Q \rev{K}}{2} = Q \wedge K$. This represents the area and orientation of the plane spanned by $Q$ and $K$.
    \item \textbf{Torque Modulation:} In our implementation, we incorporate the bivector magnitude $\|\grade{Q \rev{K}}{2}\|$ directly into the attention score (see Eq.~\ref{eq:gpa_score}). This allows the model to prioritize interactions not just by proximity (scalar) but by the orientation and ``torque'' of the plane spanned by the interacting entities. Unlike Euclidean transformers which are blind to relative orientation, GPA attends to the full geometric configuration.
\end{enumerate}

\textbf{Conclusion:} GPA is a generalization of attention where the ``alignment'' is not just scalar similarity but the \textbf{angular torque} required to align two geometric objects.

\section{Discussion and Broader Scope}
\label{app:discussion}

\subsection{Limitation Analysis (Extended)}
\label{app:limitations_extended}
\textbf{Computational Overhead:} Despite our $O(L)$ scaling, the constant factor for geometric products is approximately $5-10\times$ higher than standard matrix multiplications due to current GPU architectures not being optimized for 32-dimensional register files.
\textbf{Numerical Drift:} While we prove stability in Appendix~\ref{app:stability}, floating-point errors accumulate over $T>10,000$ steps, necessitating periodic re-normalization.
\textbf{Fixed Signature:} Versor is currently hardcoded for $\Cl$. Future work involves learning the signature metric $\eta$ (Riemannian GA).

\section{Topological Reasoning and Generalization}
\label{app:topological_reasoning}

\subsection{Algebraic Connectivity in Topological Reasoning}
\label{app:connectivity}
\textbf{Theorem (Section 6.3):} \textit{The ``Broken Snake'' task is solved by Versor through the algebraic property of null-vector orthogonality.}

\textbf{Proof:}
\begin{enumerate}
    \item \textbf{Null Vector Property:} A point $X$ is on a line or sphere $L$ if and only if $X \cdot L = 0$.
    \item \textbf{Connectivity as Orthogonality:} Two points $X_i, X_{i+1}$ are ``connected'' (infinitesimally close) if their inner product is maximal. In Conformal Geometry, as distance $\|x_1 - x_2\| \to 0$, Proof D shows that $X_1 \cdot X_2 \to 0$ (from the negative side).
    \item \textbf{Scale Invariance:} Because $X$ is a null vector ($X^2 = 0$), the relationship $X_i \cdot X_{i+1} \approx 0$ is a projectively invariant property. It does not depend on the absolute coordinates in the $16 \times 16$ or $32 \times 32$ grid.
    \item \textbf{ViT Failure:} A Vision Transformer uses positional embeddings $P \in \R^d$. When moving from a $16 \times 16$ to $32 \times 32$ grid, the learned embeddings $P_{16}$ have no mathematical relationship to $P_{32}$, leading to 50.4\% (random) accuracy.
    \item \textbf{Versor Success:} Versor learns the \textbf{relational rotor} $\Delta R$ that moves $X_i \to X_{i+1}$. Since $\Delta R$ represents a step of ``one pixel'' regardless of grid density, the logic generalizes robustly (MCC=0.993 on 32$\times$32 grids).
\end{enumerate}

\textbf{Conclusion:} Versor solves topological tasks by learning the \textit{algebraic laws} of connectivity rather than memorizing \textit{coordinate maps}.


\subsection{Complexity of Multi-Channel Scaling}
\label{app:multichannel}
\textbf{Theorem 2:} \textit{For a fixed model width $D_{model}$, Multi-Channel Versor achieves linear computational complexity $O(D_{model})$, whereas Standard Transformers scale quadratically $O(D_{model}^2)$.}

\textbf{Proof:}
Let $D_{model}$ be the total hidden dimension.
\begin{enumerate}
    \item \textbf{Standard Transformer:} A dense linear layer projects $x \in \mathbb{R}^{D_{model}}$ via $W \in \mathbb{R}^{D_{model} \times D_{model}}$.
    \begin{equation}
        \text{Cost}_{std} \propto D_{model}^2
    \end{equation}
    \item \textbf{Multi-Channel Versor:} We stack $K$ independent geometric channels of fixed dimension $d=32$, such that $D_{model} = 32K$. The geometric product operates within channels only (block-diagonal).
    \begin{equation}
        \text{Cost}_{ver} = K \times \text{Cost}(32 \times 32) = \frac{D_{model}}{32} \times C \propto O(D_{model})
    \end{equation}
\end{enumerate}

\textbf{Speedup Factor ($\eta$):}
\begin{equation}
    \eta = \frac{\text{Cost}_{std}}{\text{Cost}_{ver}} \approx \frac{D_{model}^2}{D_{model}} = D_{model}
\end{equation}

\textbf{Conclusion:} As model width increases, Versor becomes asymptotically more efficient, strictly enforcing the inductive bias that physical entities interact via shared laws (shared weights across channels) rather than arbitrary dense correlations. 

\subsection{Gradient Norm Preservation (Backward Stability)}
\label{app:gradient}

\textbf{Theorem 3 (Unitary Gradient Flow):} \textit{The backpropagation gradient through the Recursive Rotor Accumulator (RRA) preserves norm, preventing vanishing gradients independent of sequence length $L$.}

\textbf{Proof:}
Consider the recurrence relation in the RRA (Algorithm 1, line 7):
\begin{equation}
    \Psi_{t+1} = \Delta R_t \Psi_t
\end{equation}
where $\Delta R_t$ is a rotor satisfying $\Delta R_t \rev{\Delta R}_t = 1$.

During Backpropagation through Time (BPTT), we compute the gradient of the loss $\mathcal{L}$ with respect to the state $\Psi_t$ via the chain rule:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \Psi_t} = \left( \frac{\partial \Psi_{t+1}}{\partial \Psi_t} \right)^T \frac{\partial \mathcal{L}}{\partial \Psi_{t+1}}
\end{equation}

The Jacobian matrix $J_t = \frac{\partial \Psi_{t+1}}{\partial \Psi_t}$ represents the linear transformation applied to $\Psi_t$. In our architecture, this transformation is the left-multiplication by the rotor $\Delta R_t$.
\begin{equation}
    J_t \cdot v = \Delta R_t \cdot v \quad (\text{for any vector } v)
\end{equation}

Since $\Delta R_t$ is an element of the Spin group, the linear operator corresponding to rotor multiplication is \textbf{Orthogonal}.
\begin{equation}
    \det(J_t) = 1, \quad \| J_t \|_2 = 1
\end{equation}

Therefore, the magnitude of the gradient vector is preserved at each step:
\begin{equation}
    \left\| \frac{\partial \mathcal{L}}{\partial \Psi_t} \right\| = \left\| \Delta R_t^T \frac{\partial \mathcal{L}}{\partial \Psi_{t+1}} \right\| = \left\| \frac{\partial \mathcal{L}}{\partial \Psi_{t+1}} \right\|
\end{equation}

\textbf{Contrast with Standard RNNs:}
In a standard RNN ($h_{t+1} = \sigma(W h_t)$), the Jacobian depends on the weight matrix $W$.
\begin{itemize}
    \item If singular values $\sigma(W) < 1$: Gradients vanish exponentially ($\to 0$).
    \item If singular values $\sigma(W) > 1$: Gradients explode exponentially ($\to \infty$).
\end{itemize}

\textbf{Conclusion:}
By restricting the recurrence transition to the Spin manifold, Versor ensures that the gradient signal rotates rather than scales. This guarantees that error signals from step $t=L$ can propagate back to $t=0$ without degradation, enabling effective learning over extremely long horizons ($L > 10,000$). 

\subsection{Manifold Adherence of the Cayley Transform}
\label{app:cayley}

\textbf{Theorem 4:} \textit{The Cayley Transform $R = (2-B)(2+B)^{-1}$ maps any bivector $B \in \mathfrak{spin}(4,1)$ to a valid rotor $R \in Spin(4,1)$ satisfying the normalization condition $R\rev{R}=1$, provided $B$ does not have eigenvalues of $+2$.}

\textbf{Proof:}
Let $B$ be a bivector. In $\Cl$, the reversion of a bivector is $\rev{B} = -B$.
We wish to show that $R \rev{R} = 1$.

\begin{enumerate}
    \item \textbf{Define $R$ and $\rev{R}$:}
    \begin{equation}
        R = \frac{2-B}{2+B} = (2-B)(2+B)^{-1}
    \end{equation}
    \begin{equation}
        \rev{R} = \rev{(2+B)^{-1}} \rev{(2-B)}
    \end{equation}

    \item \textbf{Property of Reversion:} Reversion distributes over products with reversed order: $\rev{XY} = \rev{Y}\rev{X}$.
    Since $(2+B)$ is a sum of scalars and bivectors:
    \begin{equation}
        \rev{(2-B)} = 2 - \rev{B} = 2 - (-B) = 2+B
    \end{equation}
    \begin{equation}
        \rev{(2+B)^{-1}} = (\rev{2+B})^{-1} = (2-B)^{-1}
    \end{equation}

    \item \textbf{Substitute back:}
    \begin{equation}
        \rev{R} = (2-B)^{-1} (2+B)
    \end{equation}

    \item \textbf{Compute the Product $R\rev{R}$:}
    \begin{equation}
        R\rev{R} = \left[ (2-B)(2+B)^{-1} \right] \left[ (2-B)^{-1} (2+B) \right]
    \end{equation}

    \item \textbf{Commutativity:}
    Since $B$ commutes with itself and with scalars, $(2-B)$ and $(2+B)^{-1}$ commute.
    \begin{equation}
        R\rev{R} = (2-B) (2-B)^{-1} (2+B)^{-1} (2+B) = 1
    \end{equation}
\end{enumerate}

\textbf{Conclusion:}
The Cayley transform is a \textbf{strict map} from the Lie Algebra to the Lie Group. While it maps $B$ to $\theta$ non-linearly, it \textbf{guarantees} the output remains on the unit hypersphere. Thus, the RRA mechanism is mathematically incapable of leaving the manifold, even without the exponential map.

\section{Experimental Setup and Detailed Results}
\label{app:experimental_results}

\subsection{Experimental Details}
\label{app:details}
\subsection{Hyperparameters}
\begin{itemize}
    \item \textbf{Batch Size:} 64
    \item \textbf{Learning Rate:} $3 \times 10^{-4}$ (AdamW \citep{loshchilov2017decoupled}) with Cosine Annealing.
    \item \textbf{Layers:} 4, \textbf{Heads:} 4
    \item \textbf{Hidden Dimension:} 32 (Intrinsic to $\Cl$).
    \item \textbf{Weight Decay:} 0.01
\end{itemize}

\subsection{Data Generation}
The potential used for the 5-body problem includes a softening parameter $\epsilon=10^{-3}$ to prevent numerical singularities at collision:
\begin{equation}
    V(\mathbf{q}) = - \sum_{i < j} \frac{G m_i m_j}{\sqrt{\|\mathbf{q}_i - \mathbf{q}_j\|^2 + \epsilon^2}}
\end{equation}
Initial conditions are sampled from the solar-system distribution (one heavy mass, 4 lighter masses) such that total momentum is zero.

\subsection{Open-Source Software: \texttt{gacore}}
\label{app:gacore}

To facilitate reproducibility and industrial adoption, we release our optimized kernels as a standalone Python library: \textbf{gacore} (Geometric Algebra Core).

\subsection{Key Features}
\begin{itemize}
    \item \textbf{Universal Acceleration:} Runs on NVIDIA GPUs (via Triton), Apple Silicon (via MLX), and standard CPUs (via NumPy/Torch).
    \item \textbf{Dynamic Compilation:} JIT-compiles specialized kernels for any metric signature, not just $\Cl$.
    \item \textbf{Drop-In Compatibility:} Designed to be API-compatible with the standard \texttt{clifford} library.
\end{itemize}

\subsection{Usage Example}
The library abstracts the complexity of the bit-masked kernels behind a Pythonic API:

\begin{verbatim}
import gacore as cf
import torch

# Define metric (e.g., Cl(4,1))
signature = torch.tensor([1, 1, 1, 1, -1], device='cuda')

# Batch of 1024 multivectors (32-dim)
a = torch.randn(1024, 32, device='cuda')
b = torch.randn(1024, 32, device='cuda')

# High-speed geometric product (orders of magnitude faster)
c = cf.geometric_product(a, b, signature)
\end{verbatim}

\subsection{Quantitative Ablation Study}
\label{app:ablation}
\begin{table}[H]
\centering
\caption{Quantitative Ablation Study on chaotic N-body Task. Results show Mean MSE $\pm$ Std. Manifold Normalization is critical for convergence.}
\begin{tabular}{lccl}
\toprule
Configuration & MSE (Avg) & MSE (Std) & Stability \\
\midrule
\textbf{Full Versor} & \textbf{3.44} & 2.03 & \textbf{Stable} \\
w/o Manifold Norm & --- & --- & \textbf{Diverged (NaN)} \\
w/o Recursive Rotor & $3.44$ & $2.04$ & Stable \\
Standard Transformer & $6.38$ & 0.60 & Stable \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance Analysis}
\label{app:stats}
\textbf{Cohen's d Calculation:} We computed the standardized effect size between Versor (V) and Transformer (T) baselines:
\begin{equation}
    d = \frac{\mu_{T} - \mu_{V}}{\sigma_{\text{pooled}}} = \frac{6.609 - 5.210}{\sqrt{(6.415^2 + 6.387^2)/2}} \approx 0.22
\end{equation}
While modest, this effect size is achieved with \textbf{0.5\%} of the parameter count of the baseline.

\textbf{Confidence Intervals:} The high variance in chaotic systems results in wide CIs: $\text{CI}_{95\%} = [-2.72, 13.14]$. This reflects the inherent unpredictability of N-body chaos rather than model instability.

\subsection{Extended Hamiltonian Details}
\label{app:hamiltonian_extended}
The Hamiltonian-Versor Hybrid parameterizes the scalar energy function $H(q,p)$ using the Versor backbone.
\begin{itemize}
    \item \textbf{Input:} Concatenated state $(q, p) \in \R^{6N}$. Lifted to $\Cl$.
    \item \textbf{Backbone:} 2-layer VersorNet acting as a universal geometric function approximator.
    \item \textbf{Output:} The scalar part $\langle \Psi_{out} \rangle_0$ is treated as the Hamiltonian $\mathcal{H}$.
    \item \textbf{Integration:} We use a 4th-order Runge-Kutta symplectic integrator derived from gradients $\nabla_p \mathcal{H}$ and $-\nabla_q \mathcal{H}$.
\end{itemize}
This ensures that $\frac{d\mathcal{H}}{dt} = 0$ by construction (within integrator error), enabling the 2.4\% energy drift result.

\subsection{Extended Curriculum Results}
\label{app:curriculum}
\begin{table}[h]
\centering
\caption{Transfer gap across sequence lengths (Trained on L=50). At the $2\times$ horizon ($L=100$), Versor maintains perfect stability ($0.49\times$ gap) while GNS performance degrades by $44\times$.}
\begin{tabular}{lcccc}
\toprule
Model & Memory & Training MSE & L=100 Gap & L=150 Gap \\
\midrule
Versor (RRA) & O(1) & 0.97 & 0.49$\times$ & \textbf{29.60$\times$} \\
GNS & O(1) & 0.13 & 44.21$\times$ & 7.71$\times$ \\
Transformer & O(L) & 1.33 & 7.93$\times$ & 7.16$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Density Stress Test Details}
\label{app:scaling_capacity}
\begin{table}[H]
\centering
\caption{Density Stress Test ($N=15$). Multi-Channel Versor (48k) beats Single-Channel (109k).}
\begin{tabular}{lccl}
\toprule
Model & Params & MSE & Result \\
\midrule
Single-Channel & 109k & 12.81 & Baseline \\
Multi-Channel & \textbf{48k}  & \textbf{12.68} & \textbf{+1.0\%} \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Lie Algebra Foundations of \texorpdfstring{$\mathfrak{spin}(4,1)$}{spin(4,1)}}
\label{app:lie_algebra}
The core innovation of Versor is treating the recurrent state not as a vector in $\R^d$, but as an element of the Spin group. Here, we provide the rigorous algebraic construction of this manifold.

\subsection{Generators of the Algebra}
The Lie Algebra $\mathfrak{spin}(4,1)$ is the space of \textbf{bivectors} (grade-2 elements) in $\Cl$. It has dimension $\binom{5}{2} = 10$. A general bivector $B$ can be written as:
\begin{equation}
    B = \sum_{1 \le i < j \le 5} \beta_{ij} (e_i \wedge e_j)
\end{equation}
Physically, these generators correspond to specific symmetries:
\begin{itemize}
    \item $\{e_1 e_2, e_2 e_3, e_3 e_1\}$: \textbf{Spatial Rotations} (The $so(3)$ subalgebra).
    \item $\{e_i e_\infty\}$: \textbf{Translations} (Nilpotent directions).
    \item $\{e_o e_\infty\}$: \textbf{Dilations} (Scaling).
    \item $\{e_i e_o\}$: \textbf{Special Conformal Transformations}.
\end{itemize}
Versor exploits this structure: by learning a weight vector $W \in \R^{10}$ that projects inputs onto this bivector basis, the network learns to predict \textit{types of symmetry} (e.g., ``rotate by $x$'', ``translate by $y$'') rather than arbitrary linear maps.

\subsection{The Exponential Map vs. Cayley Map}
The mathematically ideal update rule is the exponential map $\exp: \mathfrak{spin} \to Spin$:
\begin{equation}
    R = \exp\left(-\frac{B}{2}\right) = \sum_{k=0}^\infty \frac{(-B/2)^k}{k!}
\end{equation}
However, computing the infinite series for multivectors is computationally expensive ($O(d^3)$ per step). We instead use the \textbf{Cayley Map} (Padé approximant of order (1,1)):
\begin{equation}
    R_{cayley} = \frac{1 - B/2}{1 + B/2}
\end{equation}
\textbf{Proposition:} $R_{cayley}$ approximates $\exp(-B/2)$ to second order:
\begin{align}
    \exp(-B/2) &\approx 1 - \frac{B}{2} + \frac{B^2}{8} \\
    R_{cayley} &= (2-B)(2+B)^{-1} = (2-B)\frac{1}{2}(1 - \frac{B}{2} + \dots) \approx 1 - \frac{B}{2} + \frac{B^2}{4}
\end{align}
The error term is $O(\|B\|^3)$. Since our weights are initialized to be small, this approximation acts as a high-fidelity, structure-preserving map that is computationally tractable on GPUs.

\subsection{Differential Geometry of Backpropagation}
\label{app:diff_geo_backprop}
Training Versor requires backpropagating gradients through the geometric product and Rotor updates. This is non-trivial because the parameters lie on the curved manifold $\mathcal{M} = Spin(4,1)$.

\subsection{Gradient of the Geometric Product}
Consider the loss $\mathcal{L}$ with respect to a product $Z = X Y$. Utilizing the chain rule in Clifford Algebra:
\begin{equation}
    \nabla_X \mathcal{L} = \sum_A (\nabla_Z \mathcal{L} \cdot e_A) \rev{Y} e^A
\end{equation}
where $\{e_A\}$ is the basis of the algebra.
Importantly, because the geometric product is strictly bilinear, the gradient flow is linear and stable. Unlike Softmax or Tanh which have saturating gradients ($\nabla \to 0$), the geometric product distributes gradient mass across grades without attenuation.

\subsection{Riemannian Gradient Correction}
Parameters $W$ characterize a bivector $B$ in the tangent space $T_I \mathcal{M}$. The standard Euclidean gradient $\nabla_{Euclid} \mathcal{L}$ calculated by PyTorch/Adam effectively treats the parameter space as flat.
To be strictly rigorous, one should perform a \textbf{Riemannian Retraction}:
\begin{equation}
    W_{t+1} = \text{Retr}_{W_t}(-\eta \nabla_{Riemann} \mathcal{L})
\end{equation}
However, our architecture implicitly handles this via the \textbf{Manifold Normalization} layer in the forward pass.
\begin{equation}
    \Psi_{norm} = \frac{\Psi}{\sqrt{\Psi \rev{\Psi}}}
\end{equation}
By projecting the state $\Psi$ onto the unit hypersphere at every step, we effectively project the gradient onto the tangent bundle of the Spin manifold. This proves that Manifold Normalization is not just a numerical stabilizer, but a computationally cheap proxy for Riemannian optimization.

\subsection{Algorithmic Implementation Specifications}
\label{app:algorithms}
We provide the exact algorithmic specifications for the core kernels to facilitate reproduction.

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Basis indices $i, j \in \{0 \dots 31\}$, Signature $\eta$}
\KwOut{Target index $k$, Sign $\sigma$, Metric weight $w$}
 $k \leftarrow i \oplus j$ \tcp*{XOR determines basis blade}
 $n_{swaps} \leftarrow 0$\;
 \tcp{Calculate indices to left}
 \For{$bit \leftarrow 0$ \KwTo $4$}{
  \If{$(j \gg bit) \& 1$}{
   $mask \leftarrow (\sim((1 \ll (bit+1)) - 1))$\;
   $higher\_bits \leftarrow i \& mask$\;
   $n_{swaps} \leftarrow n_{swaps} + \text{popcount}(higher\_bits)$\;
  }
 }
 $\sigma \leftarrow (-1)^{n_{swaps}}$\;
 \tcp{Metric Contraction}
 $intersection \leftarrow i \& j$\;
 $w \leftarrow 1$\;
 \If{$(intersection \gg 4) \& 1$}{
  $w \leftarrow -1$ \tcp*{e- squares to -1}
 }
 \Return{$k, \sigma \cdot w$}\;
 \caption{Bit-Masked Geometric Product Logic (Hardware Kernel)}
 \label{alg:bitmask}
\end{algorithm}

\vspace{1em}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Input stream $x_1 \dots x_L$, Bivector Params $W_B$}
\KwOut{Sequence of states $\Psi_1 \dots \Psi_L$}
 $\Psi_0 \leftarrow 1$ \tcp*{Identity Rotor}
 \For{$t \leftarrow 1$ \KwTo $L$}{
  $u_t \leftarrow \text{Linear}(x_t)$ \tcp*{Lift to algebra}
  $B_t \leftarrow \text{ProjectToBivector}(u_t, W_B)$\;
  $\Delta R_t \leftarrow (2 - B_t)(2 + B_t)^{-1}$ \tcp*{Cayley Map}
  $\Psi_t \leftarrow \Delta R_t \Psi_{t-1}$ \tcp*{Update State}
  $\Psi_t \leftarrow \Psi_t / \sqrt{\langle \Psi_t \rev{\Psi_t} \rangle_0}$ \tcp*{Manifold Norm}
 }
 \Return{$\Psi_{1 \dots L}$}
 \caption{Recursive Rotor Accumulator (Versor Core)}
\end{algorithm}

\subsection{Theoretical Roofline Analysis}
\label{app:roofline}
To understand the hardware efficiency claims, we perform a first-principles Roofline Analysis comparing Standard Transformers (Matrix Multiplication) versus Versor (Bit-Masked Clifford Product).

\subsection{Arithmetic Intensity ($\mathcal{I}$)}
Arithmetic Intensity is the ratio of Floating Point Operations (FLOPs) to Bytes moved from Memory (DRAM).
\begin{equation}
    \mathcal{I} = \frac{\text{FLOPs}}{\text{Bytes}}
\end{equation}
Higher $\mathcal{I}$ means the kernel is compute-bound (good), while lower $\mathcal{I}$ means memory-bound (bad).

\textbf{Scenario A: Standard Matrix Multiplication (Transformer)}
For a layer of width $D=32$, batch $B$:
\begin{itemize}
    \item \textbf{FLOPs:} $2 B D^2$ (approx)
    \item \textbf{Memory:} $2 B D$ (Read Input) + $D^2$ (Read Weights) + $B D$ (Write Output).
    \item Assumes large $B$: $\mathcal{I} \approx D / 2 \text{ bytes} \approx 32/2 = 16$.
\end{itemize}

\textbf{Scenario B: Sparse Table Lookup (Naive GA)}
Standard GA libraries (Cliffod, PyGAE) use a sparse multiplication table.
\begin{itemize}
    \item \textbf{FLOPs:} $N_{nonzero}$ (where $N \approx D^2$).
    \item \textbf{Memory:} Must fetch Indices (Int32) and Signs (Int8) for every non-zero operation.
    \item Overhead: 5 bytes of metadata per 1 FLOP.
    \item $\mathcal{I} \approx 1 / 5 = 0.2$.
    \item \textbf{Status:} Severely Memory Bound. This explains why standard GA is 100x slower.
\end{itemize}

\textbf{Scenario C: Versor Bit-Masked Kernel}
Our kernel computes indices/signs in registers.
\begin{itemize}
    \item \textbf{FLOPs:} $D^2 \times n$ (Logic Ops count as Ops).
    \item \textbf{Memory:} Only reads Input/Weights ($2 BD + D^2$). No metadata reads!
    \item $\mathcal{I} \approx \text{Same as Matrix Mult}$.
\end{itemize}
\textbf{Conclusion:} By converting Memory operations (Table Lookups) into ALU operations (Bitwise Logic), we move the algorithm from the memory-bound region to the compute-bound region. On modern GPUs (P100, M4), ALUs are 100x faster than Memory, resulting in the massive speedups observed.

\subsection{Extended Topological Proofs}
\label{app:topology_proofs}
\textbf{Why do Transformers fail at ``Broken Snake''?}
The ``Broken Snake'' task requires determining if two points are connected by a path.
Let the grid size be $G \times G$.

\subsection{Transformer Failure Mode}
A Vision Transformer (ViT) patches the image into tokens. It learns a position embedding $P_{i,j}$ for each coordinate.
To solve connectivity, it must learn a function $f(P_a, P_b) \to \{0, 1\}$.
\begin{itemize}
    \item \textbf{Grid Sensitivity:} The embedding $P_{i,j}$ is unique to the grid resolution. $P_{1,2}$ in a $16 \times 16$ grid has \textit{no arithmetic relationship} to $P_{1,2}$ in a $32 \times 32$ grid (which represents a different physical location).
    \item \textbf{OOD Collapse:} When testing on $32 \times 32$, the Transformer encounters position indices it has never seen. The learned lookup table $f(\cdot)$ returns garbage.
    \item \textbf{Result:} Random guessing (50.4\% accuracy).
\end{itemize}

\subsection{Versor Success Mode}
Versor does not use absolute position embeddings. It processes the stream of pixels as a chain of \textbf{Displacement Vectors} $\Delta x$.
\begin{enumerate}
    \item \textbf{Path Integration:} The RRA accumulates displacement rotors: $R_{total} = \prod \Delta R_i$.
    \item \textbf{Invariant Logic:} A ``gap'' in the snake corresponds to a jump vector $\Delta x_{gap}$ with magnitude $> 1$.
    \item \textbf{Algebraic Check:} The condition $\|\Delta x\| > 1$ is invariant to the grid size $G$. A jump is a jump, whether on a $16^2$ or $32^2$ grid.
    \item \textbf{Zero-Shot Transfer:} The model learns the rule ``If any local jump implies separation, output 0''. This rule uses only local difference arithmetic, which is resolution independent.
\end{enumerate}
This creates a \textbf{homological invariants detector}: Versor computes the 0-th Betti number (number of connected components) algebraically, effectively implementing a differentiable Union-Find algorithm on the Spin manifold.


\subsection{Formalizing the ``Euclidean Bottleneck''}
\label{app:euclidean_bottleneck}

We formally define the ``Euclidean Bottleneck'' as the incapacity of a vector-space model to represent non-abelian group actions without approximation error.

\subsection{Problem Statement}
Let $\mathcal{X}$ be a physical state space acted upon by a symmetry group $G$ (e.g., $SE(3)$).
Let $f_\theta: \mathcal{X} \to \mathcal{X}$ be a transition function (the Neural Network).
A physically valid model must be $G$-equivariant:
\begin{equation}
    f_\theta(g \cdot x) = g \cdot f_\theta(x) \quad \forall g \in G, x \in \mathcal{X}
\end{equation}

\subsection{The Vector Space Failure}
Standard Transformers embed $x$ into $\mathbb{R}^d$. The group action $g \cdot x$ in Euclidean space is represented by a matrix multiplication $M_g x$. 
However, for 3D rotations, $M_g \in SO(3)$ is a subgroup of $GL(d, \R)$.
A standard MLP layer $\sigma(Wx + b)$ is \textbf{not} equivariant to $SO(3)$ unless $W$ and $b$ satisfy specific constraints (which they do not in standard initialization).
Thus, the network must learn an approximation $\hat{f}$ such that:
\begin{equation}
    \hat{f}(M_g x) \approx M_g \hat{f}(x)
\end{equation}
This approximation requires:
\begin{enumerate}
    \item \textbf{Data Augmentation:} The training set must cover the orbit of $G$ (all possible rotations). This increases sample complexity by factor $|G|$ (infinite for continuous groups).
    \item \textbf{Parameter Waste:} The network spends capacity approximating the group law rather than the dynamics.
\end{enumerate}

\subsection{The Versor Solution}
Versor maps $x$ to the spinor bundle $\mathcal{S}$. The group action is defined by the rotor sandwich product:
\begin{equation}
    \rho(g) \psi = R \psi \rev{R}
\end{equation}
The update rule $\Psi_{t+1} = \Delta R \Psi_t$ is natively equivariant:
\begin{equation}
    (R_{ext} \Delta R \rev{R}_{ext}) (R_{ext} \Psi_t \rev{R}_{ext}) = R_{ext} (\Delta R \Psi_t) \rev{R}_{ext} = R_{ext} \Psi_{t+1} \rev{R}_{ext}
\end{equation}
Thus, Versor satisfies $f(g \cdot x) = g \cdot f(x)$ \textbf{exactly} by algebraic construction, independent of the training data. The ``Bottleneck'' (the need to learn symmetry) is removed.

\subsection{Broader Impact and Cross-Disciplinary Applications}
\label{app:impact}

While this work focuses on N-body dynamics and topology, the Versor architecture has implications across scientific domains.

\subsection{Robotics: The ``Loop Closure'' Problem}
In SLAM (Simultaneous Localization and Mapping), a robot must deduce its location from a sequence of movements.
Standard RNNs suffer from ``drift'' where accumulated errors lead to invalid rotation matrices (shearing).
Versor's manifold constraint ensures that the estimated pose is always a valid element of $SE(3)$. The \textbf{Recursive Rotor Accumulator} is isomorphic to the standard Lie Group Integrators used in control theory, but learnable. This could enable end-to-end learning of robust odometry from noisy IMU data.

\subsection{Biochemistry: Protein Folding beyond AlphaFold}
Proteins are chains of amino acids where the relative orientation of peptide planes determines the structure.
AlphaFold \citep{jumper2021highly} uses ``Invariant Point Attention'' (IPA) to explicitly model these frames.
Versor offers a more natural representation: modeling the protein backbone effectively as a ``snake'' of spinors. Our bivector attention mechanism (GPA) naturally detects ``contacts'' (scalar proximity) and ``relative alignment'' (bivector torque) between residues, potentially simplifying the AlphaFold architecture by replacing complex frame-update modules with native Clifford layers.

\subsection{Relativistic Physics}
The algebra $\Cl$ used in this paper is the Space-Time Algebra (STA) generally used in special relativity.
Unlike standard Euclidean networks, Versor can naturally model Lorentz boosts (rotations involving time $e_t$).
This suggests applications in High Energy Physics (HEP), such as jet tagging at the Large Hadron Collider, where particle decay products must be analyzed in a frame-independent manner.

\subsection{Comparison with Quaternion RNNs (QRNN)}
\label{app:quaternions}

A frequent question is the relationship between Versor and Quaternion Neural Networks (QRNNs).

\begin{table}[h]
\centering
\caption{Comparison: QRNN vs. Versor (CGA)}
\begin{tabular}{lcc}
\toprule
Feature & Quaternion RNN (QRNN) & Versor (CGA) \\
\midrule
Dimensionality & 4D ($\mathbb{H}$) & 32D ($\Cl$) \\
Geometry modeled & 3D Rotation (fixing origin) & 3D Rotation + Translation + Scaling \\
Operation & Hamilton Product & Geometric Product \\
Representational Power & Pure Rotations only & Full Conformal Group $C(3)$ \\
Applications & Attitude Control, Color Images & Dynamics, Robotics, Physics \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Theoretical Superiority:} Quaternions are a subalgebra of CGA ($\mathbb{H} \cong Spin(3) \subset Spin(4,1)$). A QRNN can theoretically model rotations, but effectively fails at \textbf{Translations}. To model a translation in a QRNN, the network must effectively ``hack'' the rotation parameters to approximate linear motion. Versor treats translation as a native rotation about the point at infinity, allowing unified learning of rigid body motion.


\subsection{Detailed Multimodal Results}
\label{app:multimodal_details}

In Section 6.3, we summarized the performance of Versor across NLP, Vision, and Graph tasks. Here, we provide the full aggregated statistics across 5 random seeds (42--46) to evaluate stability and convergence.

\begin{table}[H]
\centering
\caption{Aggregated Multimodal Performance (5 Seeds). Experiments conducted on a single M2 Ultra core. Perplexity is calculated at the character level; Accuracy is top-1; MSE is normalized geometric distance.}
\begin{tabular}{l|ccc}
\toprule
Metric & \textbf{NLP (Language)} & \textbf{Vision (Spatial)} & \textbf{Graph (Invariants)} \\
\midrule
Primary Metric & Perplexity ($\downarrow$) & Accuracy ($\uparrow$) & MSE ($\downarrow$) \\
Mean Value & \textbf{3.222} & \textbf{1.000} & \textbf{1.756} \\
Std Dev ($\pm$) & 0.006 & 0.000 & 0.176 \\
Runs ($N$) & 5 & 5 & 5 \\
Training Time (s) & 304.0 & (Aggregated) & (Aggregated) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} The zero-variance in the Vision task confirms that the geometric inductive bias for spatial feature extraction is perfectly captured by the Clifford manifold. The extremely low variance in NLP indicates that the model's sequential transitions on the Spin manifold are robust to initialization, a property often lacking in standard RNNs.

\end{document}
